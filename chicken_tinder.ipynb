{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "chicken-tinder.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/peter-ohara/chicken_tinder/blob/master/chicken_tinder.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uemZR2Tmiq0R",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "995eb52b-fb11-4cf6-d7b1-f562742a6ce5"
      },
      "source": [
        "!git clone git@github.com:peter-ohara/chicken_tinder.git"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'chicken_tinder'...\n",
            "Host key verification failed.\r\n",
            "fatal: Could not read from remote repository.\n",
            "\n",
            "Please make sure you have the correct access rights\n",
            "and the repository exists.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ob8CF6JOg0cF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350
        },
        "outputId": "7e4338d6-1589-4b54-a1fc-0d1903293253"
      },
      "source": [
        "# Imports here\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch import optim\n",
        "from torchvision import datasets, transforms\n",
        "import torchvision.models as models\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "\n",
        "import uuid\n",
        "\n",
        "from cross_validation import img_train_test_split\n",
        "from datasets import download_images_from_url_file, cleanup_images"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-681191e8038e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0muuid\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mcross_validation\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mimg_train_test_split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdatasets\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdownload_images_from_url_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcleanup_images\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'cross_validation'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l8P3j4aTiDZ8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "normal_urls_file = 'normal_urls.txt'\n",
        "abnormal_urls_file = 'abnormal_urls.txt'\n",
        "\n",
        "img_source_dir = f'raw_data_{uuid.uuid4().hex[:8]}'\n",
        "normal_images_dir = f'{img_source_dir}/normal'\n",
        "abnormal_images_dir = f'{img_source_dir}/abnormal'\n",
        "\n",
        "test_size = 0.2\n",
        "\n",
        "# Download normal images from google\n",
        "download_images_from_url_file(normal_urls_file, normal_images_dir)\n",
        "\n",
        "# Download abnormal images from google\n",
        "download_images_from_url_file(abnormal_urls_file, abnormal_images_dir)\n",
        "\n",
        "\n",
        "# Delete corrupt files\n",
        "cleanup_images(normal_images_dir)\n",
        "cleanup_images(abnormal_images_dir)\n",
        "\n",
        "# # Split into training, validation and testing\n",
        "img_train_test_split(img_source_dir, test_size, train_dir='train', test_dir='validation_and_test')\n",
        "img_train_test_split('data/validation_and_test', 0.5, train_dir='validation', test_dir='test')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UyLm1V9nh8dF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_dir = \"./drive/My Drive/Colab Notebooks/data\"\n",
        "train_dir = data_dir + \"/train\"\n",
        "valid_dir = data_dir + \"/validation\"\n",
        "test_dir = data_dir + \"/train\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TEwJYoLmh-7B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define transforms for the training, validation, and testing sets\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.RandomRotation(45),\n",
        "    transforms.RandomResizedCrop(224),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406],\n",
        "                         [0.229, 0.224, 0.225])\n",
        "   ])\n",
        "\n",
        "valid_transform = transforms.Compose([\n",
        "    transforms.Resize(255),\n",
        "    transforms.CenterCrop(224),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406],\n",
        "                         [0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "test_transform = transforms.Compose([\n",
        "    transforms.Resize(255),\n",
        "    transforms.CenterCrop(224),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406],\n",
        "                         [0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Load the datasets with ImageFolder\n",
        "train_dataset = datasets.ImageFolder(train_dir, transform=train_transform)\n",
        "valid_dataset = datasets.ImageFolder(valid_dir, transform=valid_transform)\n",
        "test_dataset = datasets.ImageFolder(test_dir, transform=test_transform)\n",
        "\n",
        "# Using the image datasets and the trainforms, define the dataloaders\n",
        "trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "validloader = torch.utils.data.DataLoader(valid_dataset, batch_size=32, shuffle=True)\n",
        "testloader = torch.utils.data.DataLoader(test_dataset, batch_size=32, shuffle=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VtvIdwZtiCwd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cat_to_name = {\"normal\": \"normal\", \"abnormal\": \"abnormal\"}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cb2ehPhIiEtd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TODO: Build and train your network\n",
        "def build_model():\n",
        "    model = models.vgg16(pretrained=True)\n",
        "\n",
        "    for param in model.parameters():\n",
        "        param.requires_grad = False\n",
        "\n",
        "    model.classifier = nn.Sequential(nn.Linear(25088, 512),\n",
        "                                     nn.ReLU(),\n",
        "                                     nn.Dropout(p=0.2),\n",
        "                                     nn.Linear(512, 256),\n",
        "                                     nn.ReLU(),\n",
        "                                     nn.Dropout(p=0.2),\n",
        "                                     nn.Linear(256, 2),\n",
        "                                     nn.LogSoftmax(dim=1))\n",
        "    \n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KUqXbq0AiGo8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def validate(dataloader, model, criterion):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "    \n",
        "    # Disable dropouts before evaluation\n",
        "    model.eval()\n",
        "\n",
        "    running_loss = 0\n",
        "    accuracy = 0\n",
        "    for images, labels in dataloader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        \n",
        "        # Disable gradient calculations when evaluating\n",
        "        with torch.no_grad():\n",
        "            \n",
        "            # Calculate loss\n",
        "            logps = model(images)\n",
        "            running_loss += criterion(logps, labels)\n",
        "            \n",
        "            # Calculate accuracy\n",
        "            ps = torch.exp(logps)\n",
        "            top_ps, top_class = ps.topk(1, dim=1)\n",
        "            matches = top_class == labels.view(*top_class.shape)\n",
        "            accuracy += torch.mean(matches.type(torch.FloatTensor))\n",
        "\n",
        "    # Enable dropouts after evaluation\n",
        "    model.train()\n",
        "\n",
        "    loss = running_loss/len(dataloader)\n",
        "    accuracy = accuracy/len(dataloader)\n",
        "    return loss, accuracy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m9dtS4E0iIU4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = build_model()\n",
        "criterion = nn.NLLLoss()\n",
        "optimizer = optim.Adam(model.classifier.parameters(), lr=0.03)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zMRQsMKeiJ4i",
        "colab_type": "code",
        "outputId": "6dc3a618-0a68-4e76-dca4-e5e13c49d001",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 707
        }
      },
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "print_metrics_every = 1\n",
        "steps = 0\n",
        "running_loss = 0\n",
        "epochs = 5\n",
        "for epoch in range(epochs):\n",
        "    model.to(\"cuda\")\n",
        "\n",
        "    for images, labels in trainloader:\n",
        "        steps += 1\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        logps = model(images)\n",
        "        loss = criterion(logps, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "        if steps % print_metrics_every == 0:\n",
        "            training_loss = running_loss/print_metrics_every\n",
        "            validation_loss, validation_accuracy = validate(validloader, model, criterion)\n",
        "\n",
        "            print(f\"Epoch: {epoch+1}/{epochs}... Training loss: {training_loss:.3f}... Validation loss: {validation_loss:.3f}... Validation Accuracy: {validation_accuracy:.3f}...\")\n",
        "\n",
        "            running_loss = 0\n",
        "\n",
        "    model.to(\"cpu\")\n",
        "\n",
        "    model.class_to_idx = train_dataset.class_to_idx\n",
        "    checkpoint = {\n",
        "        'epoch': epoch,\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "        'loss': training_loss,\n",
        "        'class_to_idx': model.class_to_idx\n",
        "    }\n",
        "    torch.save(checkpoint, './drive/My Drive/Colab Notebooks/checkpoint.tar')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1/5... Training loss: 0.706... Validation loss: 1006.962... Validation Accuracy: 0.940...\n",
            "Epoch: 1/5... Training loss: 693.962... Validation loss: 431.271... Validation Accuracy: 0.940...\n",
            "Epoch: 1/5... Training loss: 293.635... Validation loss: 99.941... Validation Accuracy: 0.940...\n",
            "Epoch: 1/5... Training loss: 0.000... Validation loss: 60.425... Validation Accuracy: 0.940...\n",
            "Epoch: 1/5... Training loss: 0.000... Validation loss: 39.640... Validation Accuracy: 0.941...\n",
            "Epoch: 2/5... Training loss: 73.052... Validation loss: 97.867... Validation Accuracy: 0.940...\n",
            "Epoch: 2/5... Training loss: 0.000... Validation loss: 141.567... Validation Accuracy: 0.940...\n",
            "Epoch: 2/5... Training loss: 67.248... Validation loss: 120.831... Validation Accuracy: 0.944...\n",
            "Epoch: 2/5... Training loss: 43.974... Validation loss: 106.309... Validation Accuracy: 0.942...\n",
            "Epoch: 2/5... Training loss: 0.000... Validation loss: 93.112... Validation Accuracy: 0.941...\n",
            "Epoch: 3/5... Training loss: 35.899... Validation loss: 72.779... Validation Accuracy: 0.940...\n",
            "Epoch: 3/5... Training loss: 45.212... Validation loss: 118.259... Validation Accuracy: 0.946...\n",
            "Epoch: 3/5... Training loss: 70.041... Validation loss: 120.316... Validation Accuracy: 0.944...\n",
            "Epoch: 3/5... Training loss: 0.000... Validation loss: 119.545... Validation Accuracy: 0.944...\n",
            "Epoch: 3/5... Training loss: 0.000... Validation loss: 122.583... Validation Accuracy: 0.941...\n",
            "Epoch: 4/5... Training loss: 0.000... Validation loss: 119.942... Validation Accuracy: 0.942...\n",
            "Epoch: 4/5... Training loss: 0.000... Validation loss: 116.614... Validation Accuracy: 0.943...\n",
            "Epoch: 4/5... Training loss: 66.054... Validation loss: 95.744... Validation Accuracy: 0.943...\n",
            "Epoch: 4/5... Training loss: 40.047... Validation loss: 50.701... Validation Accuracy: 0.943...\n",
            "Epoch: 4/5... Training loss: 40.357... Validation loss: 20.256... Validation Accuracy: 0.892...\n",
            "Epoch: 5/5... Training loss: 129.467... Validation loss: 71.747... Validation Accuracy: 0.940...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-bac0e134b090>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msteps\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mprint_metrics_every\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m             \u001b[0mtraining_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrunning_loss\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mprint_metrics_every\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m             \u001b[0mvalidation_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_accuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalidloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Epoch: {epoch+1}/{epochs}... Training loss: {training_loss:.3f}... Validation loss: {validation_loss:.3f}... Validation Accuracy: {validation_accuracy:.3f}...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-f2468c699820>\u001b[0m in \u001b[0;36mvalidate\u001b[0;34m(dataloader, model, criterion)\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0mtop_ps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtop_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mps\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtopk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0mmatches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtop_class\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mtop_class\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m             \u001b[0maccuracy\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmatches\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;31m# Enable dropouts after evaluation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1lEU5hW7n-K1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# model.to(\"cpu\")\n",
        "# torch.save(checkpoint, './drive/My Drive/Colab Notebooks/checkpoint.tar')\n",
        "\n",
        "# dataiter = iter(testloader)\n",
        "# images, labels = dataiter.next()\n",
        "\n",
        "# traced_script_module = torch.jit.trace(model, images)\n",
        "# traced_script_module.save(\"model.pt\")\n",
        "\n",
        "!mv model.pt './drive/My Drive/Colab Notebooks'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R7jKJ1WIiLsS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TODO: Do validation on the test set\n",
        "test_loss, accuracy = validate(testloader, model, criterion)\n",
        "print(\"Test loss: {:.3f}...\".format(test_loss),\n",
        "      \"Test Accuracy: {:.3f}...\".format(accuracy))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xxunc8SdiNWm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TODO: Save the checkpoint\n",
        "# model.class_to_idx = train_dataset.class_to_idx\n",
        "# checkpoint = {\n",
        "#     'epoch': epoch,\n",
        "#     'model_state_dict': model.state_dict(),\n",
        "#     'optimizer_state_dict': optimizer.state_dict(),\n",
        "#     'loss': training_loss,\n",
        "#     'class_to_idx': model.class_to_idx\n",
        "# }\n",
        "# torch.save(checkpoint, './drive/My Drive/Colab Notebooks/checkpoint.tar')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uMi2G3L6iO3R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TODO: Write a function that loads a checkpoint and rebuilds the model\n",
        "# def load_checkpoint(filepath):\n",
        "#     model = build_model()\n",
        "#     optimizer = optim.Adam(model.classifier.parameters(), lr=0.03)\n",
        "\n",
        "#     checkpoint = torch.load(filepath)\n",
        "#     model.load_state_dict(checkpoint['model_state_dict'])\n",
        "#     optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "\n",
        "#     epoch = checkpoint['epoch']\n",
        "#     loss = checkpoint['loss']\n",
        "    \n",
        "#     model.class_to_idx = checkpoint['class_to_idx']\n",
        "#     model.eval()\n",
        "\n",
        "#     return model, optimizer, epoch, loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ccO6i3rCiTd0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TODO: Process a PIL image for use in a PyTorch model\n",
        "def process_image(image):\n",
        "    ''' Scales, crops, and normalizes a PIL image for a PyTorch model,\n",
        "        returns an Numpy array\n",
        "    '''\n",
        "\n",
        "    # Resize to 256x256    \n",
        "    width, height = image.size\n",
        "    aspect_ratio = width/height\n",
        "\n",
        "    if height > width:\n",
        "        new_width = 256\n",
        "        new_height = int(256/aspect_ratio)\n",
        "    else:\n",
        "        new_width = int(256*aspect_ratio)\n",
        "        new_height = 256\n",
        "        \n",
        "    display(image)\n",
        "    print(image.size)\n",
        "    image = image.resize((new_width, new_height))\n",
        "    display(image)\n",
        "    print(image.size)\n",
        "\n",
        "    # Centre crop to 224x224\n",
        "    width, height = image.size\n",
        "    new_width, new_height = 224, 224\n",
        "    \n",
        "    left = width//2 - new_width//2\n",
        "    upper = height//2 - new_height//2\n",
        "    right = width//2 + new_width//2\n",
        "    lower = height//2 + new_height//2\n",
        "    \n",
        "    image = image.crop((left, upper, right, lower))\n",
        "\n",
        "    # Convert to float\n",
        "    np_image = np.array(image)\n",
        "    np_image = np_image / 255\n",
        "    \n",
        "    # Normalize image\n",
        "    mean = np.array([0.485, 0.456, 0.406])\n",
        "    std = np.array([0.229, 0.224, 0.225])\n",
        "    np_image = (np_image - mean) / std\n",
        "    np_image = np.transpose(np_image, axes=[2,0,1])    \n",
        "    \n",
        "    return torch.from_numpy(np_image)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PB02nXTFiVBY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def imshow(image, ax=None, title=None):\n",
        "    \"\"\"Imshow for Tensor.\"\"\"\n",
        "    if ax is None:\n",
        "        fig, ax = plt.subplots()\n",
        "    \n",
        "    # PyTorch tensors assume the color channel is the first dimension\n",
        "    # but matplotlib assumes is the third dimension\n",
        "    image = image.numpy().transpose((1, 2, 0))\n",
        "    \n",
        "    # Undo preprocessing\n",
        "    mean = np.array([0.485, 0.456, 0.406])\n",
        "    std = np.array([0.229, 0.224, 0.225])\n",
        "    image = std * image + mean\n",
        "    \n",
        "    # Image needs to be clipped between 0 and 1 or it looks like noise when displayed\n",
        "    image = np.clip(image, 0, 1)\n",
        "    \n",
        "    ax.imshow(image)\n",
        "    \n",
        "    return ax"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JVzGzP1diWff",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TODO: Implement the code to predict the class from an image file\n",
        "def predict(image_path, model, topk=2):\n",
        "    ''' Predict the class (or classes) of an image using a trained deep learning model.\n",
        "    '''\n",
        "    image = Image.open(image_path)\n",
        "    image = process_image(image).unsqueeze(dim=0)\n",
        "    image = image.type(torch.FloatTensor)\n",
        "    logps = model(image)\n",
        "    ps = torch.exp(logps)\n",
        "    top_ps, top_class = ps.topk(topk, dim=1)\n",
        "    \n",
        "    idx_to_class = {v: k for k, v in model.class_to_idx.items()}\n",
        "    \n",
        "    top_ps = [top_p.item() for top_p in top_ps.squeeze()]\n",
        "    top_class = [idx_to_class[top_class.item()] for top_class in top_class.squeeze()]\n",
        "    return top_ps, top_class\n",
        "        \n",
        "model, optimizer, epoch, training_loss = load_checkpoint('./drive/My Drive/Colab Notebooks/checkpoint.tar')\n",
        "\n",
        "image_path  = './drive/My Drive/Colab Notebooks/chickens/normal/2.jpg'\n",
        "probs, classes = predict(image_path, model)\n",
        "print(probs)\n",
        "print(classes)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CNzSshnFiYRO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TODO: Display an image along with the top 5 classes\n",
        "fig, (ax1, ax2) = plt.subplots(figsize=(6,9), nrows=2)\n",
        "\n",
        "image = Image.open(image_path)\n",
        "image = process_image(image)\n",
        "\n",
        "ax1.set_title(cat_to_name[classes[0]])\n",
        "imshow(image, ax1)\n",
        "\n",
        "ax2.barh(np.arange(5), probs)\n",
        "ax2.set_aspect(0.2)\n",
        "ax2.set_yticks(np.arange(5))\n",
        "\n",
        "labels = [cat_to_name[klass] for klass in classes]\n",
        "ax2.set_yticklabels(labels);\n",
        "ax2.set_xlim(0, 1.1)\n",
        "\n",
        "plt.tight_layout()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}