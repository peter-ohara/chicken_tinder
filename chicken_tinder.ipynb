{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "Ob8CF6JOg0cF",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "# Imports here\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torchvision import datasets, transforms\n",
    "import torchvision.models as models\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from PIL import Image\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "UyLm1V9nh8dF",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "data_dir = \"./drive/My Drive/Colab Notebooks/data\"\n",
    "train_dir = data_dir + \"/train\"\n",
    "valid_dir = data_dir + \"/validation\"\n",
    "test_dir = data_dir + \"/train\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "TEwJYoLmh-7B",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "# Define transforms for the training, validation, and testing sets\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomRotation(45),\n",
    "    transforms.RandomResizedCrop(224),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                         [0.229, 0.224, 0.225])\n",
    "   ])\n",
    "\n",
    "valid_transform = transforms.Compose([\n",
    "    transforms.Resize(255),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                         [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.Resize(255),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                         [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Load the datasets with ImageFolder\n",
    "train_dataset = datasets.ImageFolder(train_dir, transform=train_transform)\n",
    "valid_dataset = datasets.ImageFolder(valid_dir, transform=valid_transform)\n",
    "test_dataset = datasets.ImageFolder(test_dir, transform=test_transform)\n",
    "\n",
    "# Using the image datasets and the trainforms, define the dataloaders\n",
    "trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "validloader = torch.utils.data.DataLoader(valid_dataset, batch_size=32, shuffle=True)\n",
    "testloader = torch.utils.data.DataLoader(test_dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "VtvIdwZtiCwd",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "cat_to_name = {\"normal\": \"normal\", \"abnormal\": \"abnormal\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "Cb2ehPhIiEtd",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "# TODO: Build and train your network\n",
    "def build_model():\n",
    "    model = models.vgg16(pretrained=True)\n",
    "\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    model.classifier = nn.Sequential(nn.Linear(25088, 512),\n",
    "                                     nn.ReLU(),\n",
    "                                     nn.Dropout(p=0.2),\n",
    "                                     nn.Linear(512, 256),\n",
    "                                     nn.ReLU(),\n",
    "                                     nn.Dropout(p=0.2),\n",
    "                                     nn.Linear(256, 2),\n",
    "                                     nn.LogSoftmax(dim=1))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "KUqXbq0AiGo8",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "def validate(dataloader, model, criterion):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    \n",
    "    # Disable dropouts before evaluation\n",
    "    model.eval()\n",
    "\n",
    "    running_loss = 0\n",
    "    accuracy = 0\n",
    "    for images, labels in dataloader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        # Disable gradient calculations when evaluating\n",
    "        with torch.no_grad():\n",
    "            \n",
    "            # Calculate loss\n",
    "            logps = model(images)\n",
    "            running_loss += criterion(logps, labels)\n",
    "            \n",
    "            # Calculate accuracy\n",
    "            ps = torch.exp(logps)\n",
    "            top_ps, top_class = ps.topk(1, dim=1)\n",
    "            matches = top_class == labels.view(*top_class.shape)\n",
    "            accuracy += torch.mean(matches.type(torch.FloatTensor))\n",
    "\n",
    "    # Enable dropouts after evaluation\n",
    "    model.train()\n",
    "\n",
    "    loss = running_loss/len(dataloader)\n",
    "    accuracy = accuracy/len(dataloader)\n",
    "    return loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "m9dtS4E0iIU4",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "model = build_model()\n",
    "criterion = nn.NLLLoss()\n",
    "optimizer = optim.Adam(model.classifier.parameters(), lr=0.03)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "zMRQsMKeiJ4i",
    "colab_type": "code",
    "outputId": "6dc3a618-0a68-4e76-dca4-e5e13c49d001",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 707.0
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/5... Training loss: 0.706... Validation loss: 1006.962... Validation Accuracy: 0.940...\n",
      "Epoch: 1/5... Training loss: 693.962... Validation loss: 431.271... Validation Accuracy: 0.940...\n",
      "Epoch: 1/5... Training loss: 293.635... Validation loss: 99.941... Validation Accuracy: 0.940...\n",
      "Epoch: 1/5... Training loss: 0.000... Validation loss: 60.425... Validation Accuracy: 0.940...\n",
      "Epoch: 1/5... Training loss: 0.000... Validation loss: 39.640... Validation Accuracy: 0.941...\n",
      "Epoch: 2/5... Training loss: 73.052... Validation loss: 97.867... Validation Accuracy: 0.940...\n",
      "Epoch: 2/5... Training loss: 0.000... Validation loss: 141.567... Validation Accuracy: 0.940...\n",
      "Epoch: 2/5... Training loss: 67.248... Validation loss: 120.831... Validation Accuracy: 0.944...\n",
      "Epoch: 2/5... Training loss: 43.974... Validation loss: 106.309... Validation Accuracy: 0.942...\n",
      "Epoch: 2/5... Training loss: 0.000... Validation loss: 93.112... Validation Accuracy: 0.941...\n",
      "Epoch: 3/5... Training loss: 35.899... Validation loss: 72.779... Validation Accuracy: 0.940...\n",
      "Epoch: 3/5... Training loss: 45.212... Validation loss: 118.259... Validation Accuracy: 0.946...\n",
      "Epoch: 3/5... Training loss: 70.041... Validation loss: 120.316... Validation Accuracy: 0.944...\n",
      "Epoch: 3/5... Training loss: 0.000... Validation loss: 119.545... Validation Accuracy: 0.944...\n",
      "Epoch: 3/5... Training loss: 0.000... Validation loss: 122.583... Validation Accuracy: 0.941...\n",
      "Epoch: 4/5... Training loss: 0.000... Validation loss: 119.942... Validation Accuracy: 0.942...\n",
      "Epoch: 4/5... Training loss: 0.000... Validation loss: 116.614... Validation Accuracy: 0.943...\n",
      "Epoch: 4/5... Training loss: 66.054... Validation loss: 95.744... Validation Accuracy: 0.943...\n",
      "Epoch: 4/5... Training loss: 40.047... Validation loss: 50.701... Validation Accuracy: 0.943...\n",
      "Epoch: 4/5... Training loss: 40.357... Validation loss: 20.256... Validation Accuracy: 0.892...\n",
      "Epoch: 5/5... Training loss: 129.467... Validation loss: 71.747... Validation Accuracy: 0.940...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "ignored",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-bac0e134b090>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msteps\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mprint_metrics_every\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m             \u001b[0mtraining_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrunning_loss\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mprint_metrics_every\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m             \u001b[0mvalidation_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_accuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalidloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Epoch: {epoch+1}/{epochs}... Training loss: {training_loss:.3f}... Validation loss: {validation_loss:.3f}... Validation Accuracy: {validation_accuracy:.3f}...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-f2468c699820>\u001b[0m in \u001b[0;36mvalidate\u001b[0;34m(dataloader, model, criterion)\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0mtop_ps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtop_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mps\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtopk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0mmatches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtop_class\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mtop_class\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m             \u001b[0maccuracy\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmatches\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;31m# Enable dropouts after evaluation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print_metrics_every = 1\n",
    "steps = 0\n",
    "running_loss = 0\n",
    "epochs = 5\n",
    "for epoch in range(epochs):\n",
    "    model.to(\"cuda\")\n",
    "\n",
    "    for images, labels in trainloader:\n",
    "        steps += 1\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        logps = model(images)\n",
    "        loss = criterion(logps, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        if steps % print_metrics_every == 0:\n",
    "            training_loss = running_loss/print_metrics_every\n",
    "            validation_loss, validation_accuracy = validate(validloader, model, criterion)\n",
    "\n",
    "            print(f\"Epoch: {epoch+1}/{epochs}... Training loss: {training_loss:.3f}... Validation loss: {validation_loss:.3f}... Validation Accuracy: {validation_accuracy:.3f}...\")\n",
    "\n",
    "            running_loss = 0\n",
    "\n",
    "    model.to(\"cpu\")\n",
    "\n",
    "    model.class_to_idx = train_dataset.class_to_idx\n",
    "    checkpoint = {\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'loss': training_loss,\n",
    "        'class_to_idx': model.class_to_idx\n",
    "    }\n",
    "    torch.save(checkpoint, './drive/My Drive/Colab Notebooks/checkpoint.tar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "1lEU5hW7n-K1",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "# model.to(\"cpu\")\n",
    "# torch.save(checkpoint, './drive/My Drive/Colab Notebooks/checkpoint.tar')\n",
    "\n",
    "# dataiter = iter(testloader)\n",
    "# images, labels = dataiter.next()\n",
    "\n",
    "# traced_script_module = torch.jit.trace(model, images)\n",
    "# traced_script_module.save(\"model.pt\")\n",
    "\n",
    "!mv model.pt './drive/My Drive/Colab Notebooks'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "R7jKJ1WIiLsS",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "# TODO: Do validation on the test set\n",
    "test_loss, accuracy = validate(testloader, model, criterion)\n",
    "print(\"Test loss: {:.3f}...\".format(test_loss),\n",
    "      \"Test Accuracy: {:.3f}...\".format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "Xxunc8SdiNWm",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "# TODO: Save the checkpoint\n",
    "# model.class_to_idx = train_dataset.class_to_idx\n",
    "# checkpoint = {\n",
    "#     'epoch': epoch,\n",
    "#     'model_state_dict': model.state_dict(),\n",
    "#     'optimizer_state_dict': optimizer.state_dict(),\n",
    "#     'loss': training_loss,\n",
    "#     'class_to_idx': model.class_to_idx\n",
    "# }\n",
    "# torch.save(checkpoint, './drive/My Drive/Colab Notebooks/checkpoint.tar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "uMi2G3L6iO3R",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "# TODO: Write a function that loads a checkpoint and rebuilds the model\n",
    "# def load_checkpoint(filepath):\n",
    "#     model = build_model()\n",
    "#     optimizer = optim.Adam(model.classifier.parameters(), lr=0.03)\n",
    "\n",
    "#     checkpoint = torch.load(filepath)\n",
    "#     model.load_state_dict(checkpoint['model_state_dict'])\n",
    "#     optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "\n",
    "#     epoch = checkpoint['epoch']\n",
    "#     loss = checkpoint['loss']\n",
    "    \n",
    "#     model.class_to_idx = checkpoint['class_to_idx']\n",
    "#     model.eval()\n",
    "\n",
    "#     return model, optimizer, epoch, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "ccO6i3rCiTd0",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "# TODO: Process a PIL image for use in a PyTorch model\n",
    "def process_image(image):\n",
    "    ''' Scales, crops, and normalizes a PIL image for a PyTorch model,\n",
    "        returns an Numpy array\n",
    "    '''\n",
    "\n",
    "    # Resize to 256x256    \n",
    "    width, height = image.size\n",
    "    aspect_ratio = width/height\n",
    "\n",
    "    if height > width:\n",
    "        new_width = 256\n",
    "        new_height = int(256/aspect_ratio)\n",
    "    else:\n",
    "        new_width = int(256*aspect_ratio)\n",
    "        new_height = 256\n",
    "        \n",
    "    display(image)\n",
    "    print(image.size)\n",
    "    image = image.resize((new_width, new_height))\n",
    "    display(image)\n",
    "    print(image.size)\n",
    "\n",
    "    # Centre crop to 224x224\n",
    "    width, height = image.size\n",
    "    new_width, new_height = 224, 224\n",
    "    \n",
    "    left = width//2 - new_width//2\n",
    "    upper = height//2 - new_height//2\n",
    "    right = width//2 + new_width//2\n",
    "    lower = height//2 + new_height//2\n",
    "    \n",
    "    image = image.crop((left, upper, right, lower))\n",
    "\n",
    "    # Convert to float\n",
    "    np_image = np.array(image)\n",
    "    np_image = np_image / 255\n",
    "    \n",
    "    # Normalize image\n",
    "    mean = np.array([0.485, 0.456, 0.406])\n",
    "    std = np.array([0.229, 0.224, 0.225])\n",
    "    np_image = (np_image - mean) / std\n",
    "    np_image = np.transpose(np_image, axes=[2,0,1])    \n",
    "    \n",
    "    return torch.from_numpy(np_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "PB02nXTFiVBY",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "def imshow(image, ax=None, title=None):\n",
    "    \"\"\"Imshow for Tensor.\"\"\"\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots()\n",
    "    \n",
    "    # PyTorch tensors assume the color channel is the first dimension\n",
    "    # but matplotlib assumes is the third dimension\n",
    "    image = image.numpy().transpose((1, 2, 0))\n",
    "    \n",
    "    # Undo preprocessing\n",
    "    mean = np.array([0.485, 0.456, 0.406])\n",
    "    std = np.array([0.229, 0.224, 0.225])\n",
    "    image = std * image + mean\n",
    "    \n",
    "    # Image needs to be clipped between 0 and 1 or it looks like noise when displayed\n",
    "    image = np.clip(image, 0, 1)\n",
    "    \n",
    "    ax.imshow(image)\n",
    "    \n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "JVzGzP1diWff",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "# TODO: Implement the code to predict the class from an image file\n",
    "def predict(image_path, model, topk=2):\n",
    "    ''' Predict the class (or classes) of an image using a trained deep learning model.\n",
    "    '''\n",
    "    image = Image.open(image_path)\n",
    "    image = process_image(image).unsqueeze(dim=0)\n",
    "    image = image.type(torch.FloatTensor)\n",
    "    logps = model(image)\n",
    "    ps = torch.exp(logps)\n",
    "    top_ps, top_class = ps.topk(topk, dim=1)\n",
    "    \n",
    "    idx_to_class = {v: k for k, v in model.class_to_idx.items()}\n",
    "    \n",
    "    top_ps = [top_p.item() for top_p in top_ps.squeeze()]\n",
    "    top_class = [idx_to_class[top_class.item()] for top_class in top_class.squeeze()]\n",
    "    return top_ps, top_class\n",
    "        \n",
    "model, optimizer, epoch, training_loss = load_checkpoint('./drive/My Drive/Colab Notebooks/checkpoint.tar')\n",
    "\n",
    "image_path  = './drive/My Drive/Colab Notebooks/chickens/normal/2.jpg'\n",
    "probs, classes = predict(image_path, model)\n",
    "print(probs)\n",
    "print(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "CNzSshnFiYRO",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "# TODO: Display an image along with the top 5 classes\n",
    "fig, (ax1, ax2) = plt.subplots(figsize=(6,9), nrows=2)\n",
    "\n",
    "image = Image.open(image_path)\n",
    "image = process_image(image)\n",
    "\n",
    "ax1.set_title(cat_to_name[classes[0]])\n",
    "imshow(image, ax1)\n",
    "\n",
    "ax2.barh(np.arange(5), probs)\n",
    "ax2.set_aspect(0.2)\n",
    "ax2.set_yticks(np.arange(5))\n",
    "\n",
    "labels = [cat_to_name[klass] for klass in classes]\n",
    "ax2.set_yticklabels(labels);\n",
    "ax2.set_xlim(0, 1.1)\n",
    "\n",
    "plt.tight_layout()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "chicken-tinder.ipynb",
   "version": "0.3.2",
   "provenance": [],
   "collapsed_sections": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
